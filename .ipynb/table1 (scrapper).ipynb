{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd06296",
   "metadata": {},
   "source": [
    "# Importing Basic Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080e125",
   "metadata": {},
   "source": [
    "# Importing Scrapping Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c248ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait     \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce5531",
   "metadata": {},
   "source": [
    "# Importing Url Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fc3ae",
   "metadata": {},
   "source": [
    "# Driver Initialization and Calling Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_headers = {'USER-AGENT':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'} # initializing header to avoid error 403 access denied\n",
    "\n",
    "mychrome = r'\"C:\\Users\\harsh\\Downloads\\chromedriver_win32\\chromedriver.exe\"' # Initializing chrome browser\n",
    "driver = webdriver.Chrome(mychrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386a868",
   "metadata": {},
   "source": [
    "# Scrapping The the Main Page to extract links and Name of the Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a1828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_55972\\2510718491.py:24: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(mychrome)\n"
     ]
    }
   ],
   "source": [
    "restaurant_links = []\n",
    "\n",
    "driver.get('https://www.zomato.com/bangalore/restaurants') # passsing url on chrome\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n",
    "\n",
    "# Get scroll height\n",
    "curr_height = driver.execute_script(\"return document.body.scrollHeight\") # initializing scroller and getting current scrool height\n",
    "\n",
    "for i in range(0,20):    # scrolling for 20 times\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # initiates scroller\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    curr_height = new_height # update new height\n",
    "    i+=1\n",
    "    #print(i)\n",
    "    \n",
    "soup = BeautifulSoup(driver.page_source,'html.parser') # This will return the html script \n",
    "link_class = soup.findAll('div', attrs={'class':'jumbo-tracker'}) # here jumbotracker is the common class between all restaurants \n",
    "#print(link_class) \n",
    "cnt = 0\n",
    "for link in link_class:\n",
    "    urls = link.find('a') # since url are placed in href attributes of <a> class</a> hence we find a in every iteration of link_class  \n",
    "    \n",
    "    try: # here if we dont do exception handling the code will throw an exception the ver moment it reaches a url with no href\n",
    "        if 'href' in urls.attrs:\n",
    "            cnt+=1\n",
    "            restaurant_links.append(urljoin('https://www.zomato.com/',urls.get('href'))) # urljoin joins them and gets appended to list\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "restaurant_tag = [] # to store ratings of the restaurant\n",
    "cnt2 = 0\n",
    "tag_class = soup.findAll('div', attrs={'class':'sc-1q7bklc-1 cILgox'})\n",
    "for tag in range(9,len(tag_class)):\n",
    "    cnt2+=1\n",
    "    restaurant_tag.append(tag_class[tag].string) # Tag.string gives us the content present in the div tag\n",
    "    \n",
    "restaurant_name = [] # to store name of the rstaurant\n",
    "b = 0\n",
    "class_name = soup.findAll('h4')\n",
    "for name in class_name:\n",
    "    restaurant_name.append(name.string) # h4 tag consist of heading in this case is name of restaurants\n",
    "\n",
    "cuisine_list = []\n",
    "rpcn = 0\n",
    "cuisine_class = soup.findAll('p', attrs={'class': 'sc-1hez2tp-0 sc-imDdex AYTKB'})\n",
    "\n",
    "for cuisine in cuisine_class:\n",
    "    rpcn+=1\n",
    "    cuisine_list.append(''.join([cuisine.getText()])) # using join as it has a para of text that is to be scrapped as a single text so that it can be mapped with respt restaurant \n",
    "    \n",
    "    \n",
    "price_for_one_list = []\n",
    "rpc = 0\n",
    "price_class = soup.findAll('p', attrs={'class': 'sc-1hez2tp-0 sc-imDdex djnfgb'}) # to find all p tag with given class and as its paragraph tag so it consist of pragraphs\n",
    "\n",
    "for price in price_class:\n",
    "    rpc+=1\n",
    "    price_for_one_list.append(''.join([price.getText()])) # getting all the text in list using get_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f68537",
   "metadata": {},
   "source": [
    "# Appending extracted restaurant name in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_restaurant_name = []\n",
    "c=0\n",
    "for i in range(9,len(restaurant_name)):\n",
    "    c+=1\n",
    "    final_restaurant_name.append(restaurant_name[i])\n",
    "print(final_restaurant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03664f40",
   "metadata": {},
   "source": [
    "# Appending extracted restaurant links in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "restaurant_New_links = []\n",
    "for i in range(21,len(restaurant_links)):\n",
    "    a+=1\n",
    "    restaurant_New_links.append(restaurant_links[i])\n",
    "\n",
    "print(restaurant_New_links)\n",
    "len(restaurant_New_links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36a86d",
   "metadata": {},
   "source": [
    "# Creating Dataframe and storing as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintable_dataframe = pd.DataFrame({'links':restaurant_New_links,'Name':final_restaurant_name, 'tags':restaurant_tag, 'price for One':price_for_one_list, 'cuisine':cuisine_list})\n",
    "table1_dataframe # creating datafrome and converting to csv file\n",
    "maintable_dataframe.to_csv('Maintable.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
